{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim import corpora, models\n",
    "from sklearn.cluster import KMeans as sk_kmeans\n",
    "from nltk.cluster.kmeans import KMeansClusterer as nl_kmeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.graphics', 'rec.autos', 'sci.med',  'talk.politics.mideast']\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "Y = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20docs1Dif.txt'\n",
    "file = open(name, 'r')\n",
    "corpus = [line.strip() for line in file]\n",
    "file.close()\n",
    "corpus = [doc.split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_documents = [simple_preprocess(text) for text in newsgroups.data]\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.5489701329423775),\n",
       " (5, 0.5461610091086941),\n",
       " (6, 0.5937263159349457),\n",
       " (7, 0.5540937571223997),\n",
       " (8, 0.48244870246686217),\n",
       " (9, 0.5424293566345674),\n",
       " (10, 0.5873376761165991),\n",
       " (11, 0.5391533442406428),\n",
       " (12, 0.5153215253455183),\n",
       " (13, 0.5544305095214209),\n",
       " (14, 0.5455731982181954),\n",
       " (15, 0.5233916270482989),\n",
       " (16, 0.5065380640880058),\n",
       " (17, 0.5270722892847023),\n",
       " (18, 0.5319508353487308),\n",
       " (19, 0.5214311385356593)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez = []\n",
    "for i in range(4, 20):\n",
    "    lda_model = models.LdaModel(bow_corpus, num_topics=i, id2word=dictionary, passes=15)\n",
    "    coh = models.CoherenceModel(lda_model,texts=corpus, coherence='c_v')\n",
    "    rez.append((i,coh.get_coherence()))\n",
    "rez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(bow_corpus, num_topics=4, id2word=dictionary)\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    " print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_vectors = np.array([])\n",
    "for doc_bow in bow_corpus:\n",
    " document_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    " document_topic_vector = [topic_prob for _, topic_prob in document_topics]\n",
    " document_topic_vectors = np.append(document_topic_vectors, document_topic_vector)\n",
    "X = np.reshape(document_topic_vectors, (3893,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 30\n",
    "a_rand = np.zeros((2,iter))\n",
    "v_measure = np.zeros((2,iter))\n",
    "mutual = np.zeros((2,iter))\n",
    "fowlkes = np.zeros((2,iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iter):\n",
    "    eucl_pred = sk_kmeans(n_clusters=4, init='k-means++', n_init='auto').fit_predict(X)\n",
    "    a_rand[0][i] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure[0][i] = metrics.v_measure_score(Y,eucl_pred)\n",
    "    mutual[0][i] = metrics.adjusted_mutual_info_score(Y,eucl_pred)\n",
    "    fowlkes[0][i] = metrics.fowlkes_mallows_score(Y, eucl_pred)\n",
    "\n",
    "    nl_clusterer = nl_kmeans(4, distance=cosine_distance, avoid_empty_clusters=True)\n",
    "    cos_pred = nl_clusterer.cluster(X, assign_clusters=True)\n",
    "    cos_pred = np.array(cos_pred)\n",
    "    a_rand[1][i] = metrics.rand_score(Y, cos_pred)\n",
    "    v_measure[1][i] = metrics.v_measure_score(Y, cos_pred)\n",
    "    mutual[1][i] = metrics.adjusted_mutual_info_score(Y, cos_pred)\n",
    "    fowlkes[1][i] = metrics.fowlkes_mallows_score(Y, cos_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rand_h = np.zeros((2,4))\n",
    "v_measure_h = np.zeros((2,4))\n",
    "mutual_h = np.zeros((2,4))\n",
    "fowlkes_h = np.zeros((2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = ['ward', 'complete', 'average', 'single']\n",
    "for i,link in enumerate(linkage):\n",
    "    hierachical = AgglomerativeClustering(n_clusters=4, linkage=link).fit(X)\n",
    "    eucl_pred = hierachical.labels_\n",
    "    a_rand_h[0][i] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure_h[0][i] = metrics.adjusted_rand_score(Y, eucl_pred)\n",
    "    mutual_h[0][i] = metrics.homogeneity_score(Y, eucl_pred)\n",
    "    fowlkes_h[0][i] = metrics.completeness_score(Y, eucl_pred)\n",
    "linkage = ['complete', 'average', 'single']\n",
    "for i,link in enumerate(linkage):\n",
    "    hierachical = AgglomerativeClustering(n_clusters=4, linkage=link, metric='cosine').fit(X)\n",
    "    eucl_pred = hierachical.labels_\n",
    "    a_rand_h[1][i+1] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure_h[1][i+1] = metrics.adjusted_rand_score(Y, eucl_pred)\n",
    "    mutual_h[1][i+1] = metrics.homogeneity_score(Y, eucl_pred)\n",
    "    fowlkes_h[1][i+1] = metrics.completeness_score(Y, eucl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rand = [0, np.min(a_rand[0]), np.mean(a_rand[0]), np.max(a_rand[0]), 0, np.min(a_rand[1]), np.mean(a_rand[1]), np.max(a_rand[1]),\n",
    "          0, *a_rand_h[0], 0, *a_rand_h[1]]\n",
    "v_measure = [0, np.min(v_measure[0]), np.mean(v_measure[0]), np.max(v_measure[0]), 0, np.min(v_measure[1]), np.mean(v_measure[1]), np.max(v_measure[1]),\n",
    "             0, *v_measure_h[0], 0, *v_measure_h[1]] \n",
    "mutual = [0, np.min(mutual[0]), np.mean(mutual[0]), np.max(mutual[0]), 0, np.min(mutual[1]), np.mean(mutual[1]), np.max(mutual[1]),\n",
    "          0, *mutual_h[0], 0, *mutual_h[1]]\n",
    "fowlkes = [0, np.min(fowlkes[0]), np.mean(fowlkes[0]), np.max(fowlkes[0]), 0, np.min(fowlkes[1]), np.mean(fowlkes[1]), np.max(fowlkes[1]), \n",
    "           0, *fowlkes_h[0], 0, *fowlkes_h[1]]\n",
    "table = pd.DataFrame({'a_rand': a_rand, 'v_measure': v_measure, 'mutual': mutual, 'fowlkes': fowlkes},\n",
    "                     index=['k-means euclid', 'min', 'avrg', 'max', 'k-means cosine', 'min', 'avrg', 'max', \n",
    "                            'hierarchical euclid', 'ward', 'complete', 'average', 'single', 'hierarchical cosine', 'ward', 'complete', 'average', 'single'])\n",
    "table.to_excel('LDA20news.xlsx', float_format=\"%.2f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
