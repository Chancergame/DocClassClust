{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim import corpora, models\n",
    "from sklearn.cluster import KMeans as sk_kmeans\n",
    "from nltk.cluster.kmeans import KMeansClusterer as nl_kmeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('20docs3Dif.txt', 'r')\n",
    "corpus = [line.strip() for line in file]\n",
    "file.close()\n",
    "corpus = [doc.split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.graphics', 'rec.autos', 'sci.med',  'talk.politics.mideast']\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "Y = newsgroups.target\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.6755414728581919),\n",
       " (5, 0.5897825435029906),\n",
       " (6, 0.4931694608220445),\n",
       " (7, 0.5510627611209156),\n",
       " (8, 0.5226453624951066),\n",
       " (9, 0.6112757576026088),\n",
       " (10, 0.5578878588790583),\n",
       " (11, 0.5776261710106447),\n",
       " (12, 0.5450498878960249),\n",
       " (13, 0.5303916721748796),\n",
       " (14, 0.5148667454797353),\n",
       " (15, 0.5085542715760716),\n",
       " (16, 0.44833471536493885),\n",
       " (17, 0.500654128797894),\n",
       " (18, 0.48193362078291835),\n",
       " (19, 0.4420444225127686)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez = []\n",
    "for i in range(4, 20):\n",
    "    lsi_model = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=i)\n",
    "    coh = models.CoherenceModel(lsi_model, texts=corpus, coherence='c_v')\n",
    "    rez.append((i,coh.get_coherence()))\n",
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.466*\"image\" + 0.441*\"jpeg\" + 0.305*\"file\" + 0.198*\"format\" + 0.181*\"edu\" + 0.178*\"gif\" + 0.176*\"color\" + 0.140*\"graphic\" + 0.133*\"program\" + 0.126*\"version\"')\n",
      "(1, '0.468*\"jpeg\" + -0.361*\"edu\" + -0.244*\"graphic\" + -0.186*\"pub\" + 0.171*\"gif\" + -0.168*\"data\" + -0.140*\"com\" + -0.135*\"mail\" + -0.132*\"c\" + 0.131*\"color\"')\n",
      "(2, '-0.319*\"people\" + -0.177*\"armenian\" + -0.160*\"time\" + -0.156*\"child\" + -0.154*\"azerbaijani\" + -0.142*\"woman\" + -0.141*\"year\" + -0.141*\"apartment\" + 0.120*\"edu\" + -0.114*\"told\"')\n",
      "(3, '0.256*\"image\" + 0.184*\"hiv\" + -0.180*\"edu\" + 0.173*\"data\" + 0.173*\"health\" + -0.162*\"graphic\" + -0.156*\"people\" + 0.144*\"cancer\" + -0.128*\"jpeg\" + 0.125*\"page\"')\n",
      "(4, '-0.393*\"image\" + 0.199*\"jpeg\" + 0.195*\"hiv\" + -0.184*\"data\" + 0.177*\"health\" + 0.155*\"edu\" + 0.149*\"cancer\" + -0.138*\"tool\" + 0.127*\"patient\" + 0.126*\"disease\"')\n",
      "(5, '-0.383*\"jew\" + -0.372*\"turkish\" + -0.274*\"adl\" + -0.199*\"turkey\" + -0.160*\"jewish\" + -0.149*\"bullock\" + -0.137*\"nazi\" + -0.136*\"ottoman\" + 0.124*\"hiv\" + -0.112*\"university\"')\n",
      "(6, '0.554*\"adl\" + 0.301*\"bullock\" + -0.285*\"turkish\" + -0.212*\"jew\" + 0.199*\"gerard\" + 0.162*\"group\" + -0.152*\"turkey\" + 0.134*\"information\" + 0.131*\"francisco\" + 0.128*\"san\"')\n",
      "(7, '-0.496*\"com\" + -0.480*\"edu\" + -0.210*\"image\" + 0.187*\"graphic\" + 0.143*\"data\" + 0.138*\"pub\" + 0.136*\"ftp\" + 0.112*\"package\" + -0.106*\"c\" + 0.103*\"sgi\"')\n",
      "(8, '0.324*\"image\" + -0.287*\"com\" + -0.240*\"data\" + -0.177*\"edu\" + 0.155*\"graphic\" + -0.123*\"vertex\" + -0.118*\"jpeg\" + 0.117*\"mail\" + -0.115*\"grass\" + 0.114*\"ray\"')\n",
      "(9, '0.358*\"cancer\" + -0.247*\"use\" + 0.238*\"hiv\" + -0.166*\"state\" + 0.158*\"aid\" + -0.141*\"health\" + -0.137*\"car\" + -0.136*\"tobacco\" + -0.134*\"year\" + -0.128*\"e\"')\n",
      "(10, '-0.495*\"x\" + -0.438*\"z\" + -0.306*\"den\" + -0.291*\"p2\" + -0.289*\"p3\" + -0.250*\"p1\" + -0.166*\"radius\" + -0.118*\"f\" + 0.095*\"jew\" + -0.094*\"p23\"')\n",
      "(11, '-0.470*\"kuwait\" + -0.300*\"car\" + -0.165*\"oil\" + -0.159*\"al\" + 0.144*\"jew\" + -0.142*\"azerbaijan\" + -0.133*\"cancer\" + -0.129*\"tire\" + 0.124*\"health\" + -0.123*\"brake\"')\n",
      "(12, '0.396*\"kuwait\" + -0.382*\"car\" + -0.205*\"jew\" + 0.171*\"azerbaijan\" + -0.157*\"tire\" + -0.151*\"brake\" + 0.148*\"armenian\" + 0.139*\"al\" + -0.117*\"fluid\" + -0.107*\"system\"')\n",
      "(13, '-0.489*\"kuwait\" + 0.377*\"azerbaijan\" + 0.208*\"armenian\" + -0.200*\"jew\" + 0.169*\"azeri\" + 0.168*\"armenia\" + -0.168*\"al\" + 0.162*\"ar\" + 0.133*\"baku\" + -0.122*\"sheikh\"')\n",
      "(14, '0.453*\"hiv\" + -0.373*\"cancer\" + -0.221*\"center\" + 0.207*\"aid\" + -0.200*\"university\" + -0.156*\"ed\" + 0.129*\"vaccine\" + 0.126*\"car\" + 0.114*\"cesarean\" + 0.108*\"turkish\"')\n",
      "(15, '-0.475*\"ed\" + -0.257*\"istanbul\" + -0.209*\"university\" + 0.197*\"turkish\" + -0.164*\"hiv\" + 0.159*\"azerbaijan\" + -0.149*\"v\" + -0.149*\"ankara\" + -0.146*\"professor\" + 0.137*\"cancer\"')\n",
      "(16, '-0.326*\"vitamin\" + -0.311*\"ed\" + 0.228*\"car\" + -0.222*\"israel\" + 0.167*\"university\" + 0.162*\"center\" + -0.131*\"arab\" + 0.127*\"child\" + -0.126*\"retinol\" + 0.124*\"cancer\"')\n",
      "(17, '-0.380*\"ed\" + 0.366*\"vitamin\" + -0.287*\"israel\" + -0.175*\"arab\" + -0.175*\"jew\" + 0.143*\"retinol\" + 0.124*\"keyboard\" + 0.115*\"turkish\" + -0.114*\"azerbaijan\" + 0.114*\"pm\"')\n",
      "(18, '-0.513*\"keyboard\" + -0.252*\"gopher\" + 0.251*\"vitamin\" + -0.239*\"pc\" + -0.150*\"key\" + -0.142*\"support\" + -0.133*\"search\" + 0.128*\"car\" + -0.120*\"price\" + -0.102*\"software\"')\n",
      "(19, '-0.250*\"tobacco\" + 0.241*\"child\" + -0.198*\"health\" + -0.189*\"azerbaijan\" + -0.174*\"smokeless\" + 0.158*\"vitamin\" + -0.139*\"coli\" + 0.135*\"safety\" + -0.126*\"o157\" + -0.126*\"h7\"')\n"
     ]
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=20)\n",
    "lsi_topics = lsi_model.print_topics(num_topics=20, num_words=10)\n",
    "for topic in lsi_topics:\n",
    " print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_vectors = []\n",
    "X = np.array([])\n",
    "for doc_bow in bow_corpus:\n",
    "    document_topic_vector = lsi_model[doc_bow]\n",
    "    document_topic_vectors.append(document_topic_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([])\n",
    "for vec in document_topic_vectors:\n",
    "    a = np.array([])\n",
    "    if len(vec) < 20:\n",
    "        a = np.array([1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10])\n",
    "    else:    \n",
    "        for toup in vec:\n",
    "            a = np.append(a, toup[1])\n",
    "    X = np.append(X, a)\n",
    "X = np.reshape(X, (3893,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 30\n",
    "a_rand = np.zeros((2,iter))\n",
    "v_measure = np.zeros((2,iter))\n",
    "mutual = np.zeros((2,iter))\n",
    "fowlkes = np.zeros((2,iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iter):\n",
    "    eucl_pred = sk_kmeans(n_clusters=4, init='k-means++', n_init='auto').fit_predict(X)\n",
    "    a_rand[0][i] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure[0][i] = metrics.v_measure_score(Y,eucl_pred)\n",
    "    mutual[0][i] = metrics.adjusted_mutual_info_score(Y,eucl_pred)\n",
    "    fowlkes[0][i] = metrics.fowlkes_mallows_score(Y, eucl_pred)\n",
    "\n",
    "    nl_clusterer = nl_kmeans(4, distance=cosine_distance, avoid_empty_clusters=True)\n",
    "    cos_pred = nl_clusterer.cluster(X, assign_clusters=True)\n",
    "    cos_pred = np.array(cos_pred)\n",
    "    a_rand[1][i] = metrics.rand_score(Y, cos_pred)\n",
    "    v_measure[1][i] = metrics.v_measure_score(Y, cos_pred)\n",
    "    mutual[1][i] = metrics.adjusted_mutual_info_score(Y, cos_pred)\n",
    "    fowlkes[1][i] = metrics.fowlkes_mallows_score(Y, cos_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rand_h = np.zeros((2,4))\n",
    "v_measure_h = np.zeros((2,4))\n",
    "mutual_h = np.zeros((2,4))\n",
    "fowlkes_h = np.zeros((2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = ['ward', 'complete', 'average', 'single']\n",
    "for i,link in enumerate(linkage):\n",
    "    hierachical = AgglomerativeClustering(n_clusters=4, linkage=link).fit(X)\n",
    "    eucl_pred = hierachical.labels_\n",
    "    a_rand_h[0][i] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure_h[0][i] = metrics.adjusted_rand_score(Y, eucl_pred)\n",
    "    mutual_h[0][i] = metrics.homogeneity_score(Y, eucl_pred)\n",
    "    fowlkes_h[0][i] = metrics.completeness_score(Y, eucl_pred)\n",
    "linkage = ['complete', 'average', 'single']\n",
    "for i,link in enumerate(linkage):\n",
    "    hierachical = AgglomerativeClustering(n_clusters=4, linkage=link, metric='cosine').fit(X)\n",
    "    eucl_pred = hierachical.labels_\n",
    "    a_rand_h[1][i+1] = metrics.rand_score(Y, eucl_pred)\n",
    "    v_measure_h[1][i+1] = metrics.adjusted_rand_score(Y, eucl_pred)\n",
    "    mutual_h[1][i+1] = metrics.homogeneity_score(Y, eucl_pred)\n",
    "    fowlkes_h[1][i+1] = metrics.completeness_score(Y, eucl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rand = [0, np.min(a_rand[0]), np.mean(a_rand[0]), np.max(a_rand[0]), 0, np.min(a_rand[1]), np.mean(a_rand[1]), np.max(a_rand[1]),\n",
    "          0, *a_rand_h[0], 0, *a_rand_h[1]]\n",
    "v_measure = [0, np.min(v_measure[0]), np.mean(v_measure[0]), np.max(v_measure[0]), 0, np.min(v_measure[1]), np.mean(v_measure[1]), np.max(v_measure[1]),\n",
    "             0, *v_measure_h[0], 0, *v_measure_h[1]] \n",
    "mutual = [0, np.min(mutual[0]), np.mean(mutual[0]), np.max(mutual[0]), 0, np.min(mutual[1]), np.mean(mutual[1]), np.max(mutual[1]),\n",
    "          0, *mutual_h[0], 0, *mutual_h[1]]\n",
    "fowlkes = [0, np.min(fowlkes[0]), np.mean(fowlkes[0]), np.max(fowlkes[0]), 0, np.min(fowlkes[1]), np.mean(fowlkes[1]), np.max(fowlkes[1]), \n",
    "           0, *fowlkes_h[0], 0, *fowlkes_h[1]]\n",
    "table = pd.DataFrame({'a_rand': a_rand, 'v_measure': v_measure, 'mutual': mutual, 'fowlkes': fowlkes},\n",
    "                     index=['k-means euclid', 'min', 'avrg', 'max', 'k-means cosine', 'min', 'avrg', 'max', \n",
    "                            'hierarchical euclid', 'ward', 'complete', 'average', 'single', 'hierarchical cosine', 'ward', 'complete', 'average', 'single'])\n",
    "table.to_excel('LSI20news.xlsx', float_format=\"%.2f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
